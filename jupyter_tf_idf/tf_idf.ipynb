{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pylab\n",
    "import csv\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем метод, для удаления подстрок из черного списка и замены слов на их начальную форму "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(strings):\n",
    "    functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP', 'NUMB'}\n",
    "    black_list = [',', '.', '!','?','-',';','\\'','/','(',')','[',']','{','}','–', '+']\n",
    "    strings_list = []\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    # цикл для удаления подстрок из black_list \n",
    "    for i in range(len(strings)):\n",
    "        for j in range(len(black_list)):\n",
    "            strings[i] = strings[i].replace(black_list[j],'')\n",
    "        temp = strings[i].lower().split();\n",
    "        strings_list.append([])\n",
    "        # цикл для замены слова на его начальную форму \n",
    "        for j in range(len(temp)):\n",
    "            p = morph.parse(temp[j])\n",
    "            if len(p)>0 and p[0].tag.POS not in functors_pos:\n",
    "                strings_list[len(strings_list)-1].append(p[0].normal_form)\n",
    "            else:\n",
    "                strings_list[len(strings_list)-1].append(temp[j])\n",
    "    return strings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_dict_writer(path, fieldnames, data):\n",
    "    \"\"\"\n",
    "    Writes a CSV file using DictWriter\n",
    "    \"\"\"\n",
    "    with open(path, \"w\", newline='') as out_file:\n",
    "        writer = csv.DictWriter(out_file, delimiter=';', fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим список текстов из библиотеки текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем содержимое выбранного документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(\"dictionary\")\n",
    "arr = []\n",
    "for k in range(len(dirs)):\n",
    "    files = os.listdir(\"dictionary/\"+dirs[k])\n",
    "    collection = \"\"\n",
    "    for j in range(len(files)):\n",
    "        with open('dictionary/'+dirs[k]+\"/\"+files[j]) as f:\n",
    "            line = f.readline()\n",
    "            \n",
    "            while line:\n",
    "                collection += line + \" \"\n",
    "                line = f.readline()\n",
    "    arr.append(collection)\n",
    "#ind = int(input())\n",
    "#with open('dictionary/'+files[ind]) as f:\n",
    "#    arr = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(arr)\n",
    "# список, содержащий списки слов каждого предложения\n",
    "letter = fix_text(arr)\n",
    "# множество всех слов\n",
    "word_set = {}\n",
    "for i in range(n):\n",
    "    word_set = set(word_set).union(set(letter[i]))\n",
    "    \n",
    "word_dict_arr = [dict.fromkeys(word_set, 0) for i in range(n)]\n",
    "# считаем количество вхождения слов в кажом предложении\n",
    "for i in range(n):\n",
    "    for word in letter[i]:\n",
    "        word_dict_arr[i][word] += 1\n",
    "\n",
    "# вычисляем tf\n",
    "def compute_tf(word_dict, l):\n",
    "    tf = {}\n",
    "    sum_nk = len(l)\n",
    "    for word, count in word_dict.items():\n",
    "        tf[word] = count / float(sum_nk)\n",
    "    return tf\n",
    "\n",
    "# вычисляем tf для каждого предложения\n",
    "tf_arr = [compute_tf(word_dict_arr[i], letter[i]) for i in range(n)]\n",
    "\n",
    "# вычисляем idf\n",
    "def compute_idf(strings_list):\n",
    "    n = len(strings_list)\n",
    "    idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "    for l in strings_list:\n",
    "        for word, count in l.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1\n",
    "\n",
    "    for word, v in idf.items():\n",
    "        idf[word] = math.log(n / float(v), 10)\n",
    "    return idf\n",
    "\n",
    "# вычисляем idf для каждого предложения\n",
    "idf = compute_idf(word_dict_arr)\n",
    "# вычисляем tf и idf\n",
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "    for word, v in tf.items():\n",
    "        tf_idf[word] = tf[word] * idf[word]\n",
    "    return tf_idf\n",
    "\n",
    "# вычисляем tf и idf для каждого предложения\n",
    "tf_idf_arr = [compute_tf_idf(tf_arr[i], idf) for i in range(n)]\n",
    "# вывод результата\n",
    "# for i in range(n):\n",
    "#     print(tf_idf_arr[i])\n",
    "my_list = []\n",
    "for word in word_set:\n",
    "    my_list.append({\" \": word})\n",
    "    for i in range(len(dirs)):\n",
    "        dirname = dirs[i]\n",
    "        my_list[len(my_list)-1][dirname] = tf_idf_arr[i][word]\n",
    "    \n",
    "dirs.insert(0,\" \")\n",
    "csv_dict_writer(\"tf_idf.csv\", dirs, my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
